{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB6oenLl1o-_"
      },
      "source": [
        "# Multi-GPU Mirrored Strategy\n",
        "\n",
        "In this ungraded lab you'll go on how to set up a Multi-GPU Mirrored Strategy.\n",
        "\n",
        "**Note:** \n",
        "- If you are running this on Coursera you'll see it gives a warning about no presence of GPU devices. \n",
        "- If you are running this in Colab make sure you have selected your `runtime` to be `GPU` for it to detect it. \n",
        "- In both these cases you'll see there's only 1 device that is available.  \n",
        "- One device is sufficient for helping you understand the these distribution strategies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "vXS8MYim6nNY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that it generally has a minimum of 8 cores, but if your GPU has\n",
        "# less, you need to set this. In this case one of my GPUs has 4 cores\n",
        "os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\""
      ],
      "metadata": {
        "id": "Px7KU9qX6nA8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the list of devices is not specified in the\n",
        "# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n",
        "# If you have *different* GPUs in your system, you probably have to set up cross_device_ops like this\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
        "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlXHgDj96m4d",
        "outputId": "1003892b-d9a9-45fb-85d6-4b3d2e0101fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of devices: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2myj8YGo6mvu",
        "outputId": "04f27be3-1c2d-4ddf-a669-1e936bd61d46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a dimension to the array -> new shape == (28, 28, 1)\n",
        "# We are doing this because the first layer in our model is a convolutional\n",
        "# layer and it requires a 4D input (batch_size, height, width, channels).\n",
        "# batch_size dimension will be added later on.\n",
        "train_images = train_images[..., None]\n",
        "test_images = test_images[..., None]"
      ],
      "metadata": {
        "id": "wutrTGBV6_qI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the images to [0, 1] range.\n",
        "train_images = train_images / np.float32(255)\n",
        "test_images = test_images / np.float32(255)"
      ],
      "metadata": {
        "id": "jc0B6qXX6_nh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch the input data\n",
        "BUFFER_SIZE = len(train_images)\n",
        "BATCH_SIZE_PER_REPLICA = 64\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
      ],
      "metadata": {
        "id": "4AQPb8RP6_lg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Datasets from the batches\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)"
      ],
      "metadata": {
        "id": "151K3UQL6_iN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Distributed Datasets from the datasets\n",
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
      ],
      "metadata": {
        "id": "BYwHxlbR6_fQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model architecture\n",
        "def create_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.Dense(10)\n",
        "    ])\n",
        "  return model"
      ],
      "metadata": {
        "id": "oh9y4nev6_bc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of model.compile, we're going to do custom training, so let's do that\n",
        "# within a strategy scope\n",
        "with strategy.scope():\n",
        "    # We will use sparse categorical crossentropy as always. But, instead of having the loss function\n",
        "    # manage the map reduce across GPUs for us, we'll do it ourselves with a simple algorithm.\n",
        "    # Remember -- the map reduce is how the losses get aggregated\n",
        "    # Set reduction to `none` so we can do the reduction afterwards and divide byglobal batch size.\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "    def compute_loss(labels, predictions):\n",
        "        # Compute Loss uses the loss object to compute the loss\n",
        "        # Notice that per_example_loss will have an entry per GPU\n",
        "        # so in this case there'll be 2 -- i.e. the loss for each replica\n",
        "        per_example_loss = loss_object(labels, predictions)\n",
        "        # You can print it to see it -- you'll get output like this:\n",
        "        # Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(48,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
        "        # Tensor(\"replica_1/sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(48,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
        "        # Note in particular that replica_0 isn't named in the weighted_loss -- the first is unnamed, the second is replica_1 etc\n",
        "        print(per_example_loss)\n",
        "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
        "\n",
        "    # We'll just reduce by getting the average of the losses\n",
        "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "    # Accuracy on train and test will be SparseCategoricalAccuracy\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "    # Optimizer will be Adam\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Create the model within the scope\n",
        "    model = create_model()"
      ],
      "metadata": {
        "id": "TReHEEh58mQM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# Training Steps Functions\n",
        "###########################\n",
        "\n",
        "# `run` replicates the provided computation and runs it\n",
        "# with the distributed input.\n",
        "@tf.function\n",
        "def distributed_train_step(dataset_inputs):\n",
        "  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
        "  # tf.print(per_replica_losses.values)\n",
        "  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "def train_step(inputs):\n",
        "  images, labels = inputs\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(images, training=True)\n",
        "    loss = compute_loss(labels, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_accuracy.update_state(labels, predictions)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "9A9nNfsa8mIt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# Test Steps Functions\n",
        "#######################\n",
        "@tf.function\n",
        "def distributed_test_step(dataset_inputs):\n",
        "  return strategy.run(test_step, args=(dataset_inputs,))\n",
        "\n",
        "def test_step(inputs):\n",
        "  images, labels = inputs\n",
        "\n",
        "  predictions = model(images, training=False)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss.update_state(t_loss)\n",
        "  test_accuracy.update_state(labels, predictions)"
      ],
      "metadata": {
        "id": "vBAgqAG78mBt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "k0VOdqKP2NEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad4cba7e-f916-483e-f184-ce98dc8339da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(64,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(64,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(32,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Epoch 1, Loss: 0.515017032623291, Accuracy: 81.2750015258789, Test Loss: 0.3826509118080139, Test Accuracy: 86.26000213623047\n",
            "Epoch 2, Loss: 0.3377319872379303, Accuracy: 87.83499908447266, Test Loss: 0.33827364444732666, Test Accuracy: 87.56999969482422\n",
            "Epoch 3, Loss: 0.2896917462348938, Accuracy: 89.50166320800781, Test Loss: 0.29443594813346863, Test Accuracy: 89.31000518798828\n",
            "Epoch 4, Loss: 0.2576424181461334, Accuracy: 90.49166107177734, Test Loss: 0.278921514749527, Test Accuracy: 89.7800064086914\n",
            "Epoch 5, Loss: 0.2320198267698288, Accuracy: 91.48999786376953, Test Loss: 0.2765443027019501, Test Accuracy: 89.9800033569336\n",
            "Epoch 6, Loss: 0.2138524353504181, Accuracy: 92.07833099365234, Test Loss: 0.2700926661491394, Test Accuracy: 90.16999816894531\n",
            "Epoch 7, Loss: 0.193742036819458, Accuracy: 92.8133316040039, Test Loss: 0.2441895753145218, Test Accuracy: 91.2699966430664\n",
            "Epoch 8, Loss: 0.17989574372768402, Accuracy: 93.2733383178711, Test Loss: 0.2656354606151581, Test Accuracy: 90.41999816894531\n",
            "Epoch 9, Loss: 0.164167582988739, Accuracy: 93.91999816894531, Test Loss: 0.25058814883232117, Test Accuracy: 91.22999572753906\n",
            "Epoch 10, Loss: 0.15153814852237701, Accuracy: 94.36333465576172, Test Loss: 0.2633656859397888, Test Accuracy: 91.05999755859375\n"
          ]
        }
      ],
      "source": [
        "###############\n",
        "# TRAINING LOOP\n",
        "###############\n",
        "\n",
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "  # Do Training\n",
        "  total_loss = 0.0\n",
        "  num_batches = 0\n",
        "  for batch in train_dist_dataset:\n",
        "    total_loss += distributed_train_step(batch)\n",
        "    num_batches += 1\n",
        "  train_loss = total_loss / num_batches\n",
        "\n",
        "  # Do Testing\n",
        "  for batch in test_dist_dataset:\n",
        "    distributed_test_step(batch)\n",
        "\n",
        "  template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \" \"Test Accuracy: {}\")\n",
        "\n",
        "  print (template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100))\n",
        "\n",
        "  test_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_accuracy.reset_states()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}